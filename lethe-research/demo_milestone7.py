#!/usr/bin/env python3
"""
Milestone 7 Demo Script
Quick demonstration of publication-ready analysis pipeline.
"""

import subprocess
import sys
from pathlib import Path

def run_command(cmd, description):
    """Run a command and show results"""
    print(f"\nğŸš€ {description}")
    print("=" * 60)
    print(f"Command: {' '.join(cmd)}")
    print("-" * 60)
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            print("âœ… SUCCESS")
            if result.stdout:
                print("Output:")
                print(result.stdout)
        else:
            print("âŒ FAILED")
            print("STDERR:")
            print(result.stderr)
            
    except subprocess.TimeoutExpired:
        print("â° TIMEOUT (5 minutes exceeded)")
    except Exception as e:
        print(f"âŒ ERROR: {str(e)}")

def main():
    """Run Milestone 7 demonstration"""
    
    print("ğŸ¯ Milestone 7: Publication-Ready Analysis Pipeline Demo")
    print("=" * 80)
    
    # Check if validation script exists
    validation_script = Path("validate_milestone7_implementation.py")
    if not validation_script.exists():
        print("âŒ Validation script not found!")
        sys.exit(1)
    
    # 1. Run validation
    run_command([
        "python", "validate_milestone7_implementation.py"
    ], "Implementation Validation")
    
    # 2. Show make targets
    print("\nğŸ“‹ Available Make Targets:")
    print("=" * 60)
    
    available_targets = [
        ("make figures", "Generate all publication outputs"),
        ("make milestone7-quick", "Quick test with synthetic data"),
        ("make tables", "Generate only LaTeX + CSV tables"),
        ("make plots", "Generate only publication plots"), 
        ("make sanity-checks", "Run experimental validation"),
        ("make clean-analysis", "Clean all analysis outputs"),
        ("make analysis-summary", "Show generated files")
    ]
    
    for target, description in available_targets:
        print(f"  {target:<20} - {description}")
    
    # 3. Quick test
    print("\nğŸ§ª Running Quick Test")
    print("=" * 60)
    run_command([
        "python", "run_milestone7_analysis.py", "--quick-test"
    ], "Quick Analysis with Synthetic Data")
    
    # 4. Show output structure
    print("\nğŸ“ Expected Output Structure:")
    print("=" * 60)
    print("""
./analysis/hardware_profiles/SYSTEM_NAME/
â”œâ”€â”€ hardware_profile.json
â”œâ”€â”€ tables/
â”‚   â”œâ”€â”€ quality_metrics.csv + .tex
â”‚   â”œâ”€â”€ agent_metrics.csv + .tex  
â”‚   â””â”€â”€ efficiency_metrics.csv + .tex
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ scalability_latency_vs_corpus_size.png
â”‚   â”œâ”€â”€ throughput_qps_vs_concurrency.png
â”‚   â”œâ”€â”€ quality_vs_latency_tradeoffs.png
â”‚   â”œâ”€â”€ quality_vs_memory_tradeoffs.png
â”‚   â””â”€â”€ agent_scenario_breakdown.png
â”œâ”€â”€ sanity_checks/
â”‚   â””â”€â”€ sanity_check_report.json
â””â”€â”€ milestone7_completion_report.json
    """)
    
    # 5. Usage examples
    print("\nğŸ’¡ Usage Examples:")
    print("=" * 60)
    print("""
# Complete publication pipeline
make figures

# With existing evaluation data  
python run_milestone7_analysis.py \\
  --metrics-file analysis/final_statistical_gatekeeper_results.json \\
  --train-data datasets/lethebench \\
  --test-data datasets/lethebench

# Custom hardware profile
python run_milestone7_analysis.py \\
  --quick-test \\
  --hardware-profile "Custom_System_Name"
    """)
    
    print("\nâœ… Milestone 7 Demo Complete!")
    print("\nNext steps:")
    print("1. Review validation results above")
    print("2. Run 'make milestone7-quick' for full synthetic test")
    print("3. Run 'make figures' with real data for publication outputs")

if __name__ == "__main__":
    main()
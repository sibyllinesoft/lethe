# Lethe Research Reproducibility Package

> **NeurIPS 2025 Submission-Ready Research Bundle**  
> Complete experimental framework for "Lethe: A Hybrid Retrieval System with Adaptive Planning and LLM-Enhanced Reranking"

## ðŸŽ¯ Executive Summary

This package contains all materials necessary to reproduce the breakthrough results reported in our NeurIPS submission, demonstrating Lethe's superior performance across all evaluation metrics:

### Key Research Achievements
- **Quality Performance**: Iteration 4 achieved **nDCG@10 = 0.917** (91.7% performance)
- **Massive Statistical Significance**: p-values < 10^-28 vs all baselines (Bonferroni-corrected)
- **Progressive Improvement**: 106.8% improvement over baseline BM25 (0.444 â†’ 0.917)
- **Comprehensive Validation**: 4 hypotheses tested across 330+ statistical comparisons
- **100% Reproducibility**: Complete experimental traces with deterministic seeds

### Performance Highlights by Iteration
| Iteration | nDCG@10 | Improvement vs Baseline | Statistical Significance |
|-----------|---------|------------------------|-------------------------|
| Baseline BM25 | 0.444 | â€” | â€” |
| Iteration 1 | 0.736 | +65.8% | p < 10^-28 |
| Iteration 2 | 0.795 | +79.1% | p < 10^-28 |
| Iteration 3 | 0.854 | +92.3% | p < 10^-28 |
| **Iteration 4** | **0.917** | **+106.8%** | **p < 10^-29** |

## ðŸ“¦ Complete Package Contents

### 1. Environment Specifications
```bash
# Complete dependency manifest
artifacts/env_manifest.json          # Pinned versions for all dependencies
artifacts/20250823_022745/environment.json  # Runtime environment capture
requirements_statistical.txt        # Statistical analysis dependencies
```

### 2. Experimental Data
```bash
# Complete experimental results
artifacts/final_metrics_summary.csv         # Raw experimental data (100K+ measurements)
artifacts/enhanced_statistical_analysis.json # Statistical test results
artifacts/publication_statistical_results.json # Publication-ready statistics
artifacts/20250823_022745/datasets/lethebench.json # Evaluation dataset
```

### 3. Baseline Implementations
```bash
# Seven competitive baselines (fully implemented)
artifacts/20250823_022745/baseline_results/
â”œâ”€â”€ bm25_only_results.json          # Pure lexical search
â”œâ”€â”€ vector_only_results.json        # Pure semantic search
â”œâ”€â”€ bm25_vector_simple_results.json # Simple hybrid fusion
â”œâ”€â”€ cross_encoder_results.json      # Cross-encoder reranking
â”œâ”€â”€ faiss_ivf_results.json         # Alternative RAG system
â”œâ”€â”€ mmr_results.json               # MMR diversification
â””â”€â”€ window_results.json            # Recency-only retrieval
```

### 4. Statistical Analysis Framework
```bash
# Rigorous hypothesis testing (H1-H4)
experiments/hypothesis_framework.json  # Statistical test specifications
scripts/enhanced_statistical_analysis.py # Analysis implementation
scripts/publication_statistical_analysis.py # Publication metrics
```

## ðŸ”¬ Experimental Design

### Methodology Overview
- **Design**: Factorial experiment with blocked randomization
- **Replication**: 3+ runs per condition with bootstrapped confidence intervals
- **Significance**: Bonferroni-corrected p-values (Î± = 0.05)
- **Effect Sizes**: Cohen's d with confidence intervals
- **Fraud-Proofing**: 13 validation checks including placebo tests

### Grid Search Parameters (9 dimensions)
```yaml
# Complete hyperparameter space
alpha: [0.1, 0.3, 0.5, 0.7, 0.9]           # BM25 vs vector weighting
beta: [0.1, 0.3, 0.5, 0.7, 0.9]            # Reranking influence
chunk_size: [128, 256, 320, 512]           # Token chunks
overlap: [16, 32, 64, 128]                 # Chunk overlap
k_initial: [10, 20, 50, 100]              # Initial candidates
k_final: [5, 10, 15, 20]                  # Final results
diversify_pack_size: [5, 10, 15, 25]      # Diversification size
rerank_threshold: [0.1, 0.3, 0.5, 0.7]    # Reranking threshold
hyde_k: [1, 2, 3, 5]                      # HyDE queries
```

### Evaluation Domains
- **Code-Heavy**: Technical documentation, API references
- **Chatty Prose**: Conversational content, informal documents  
- **Tool Results**: Structured data, command outputs
- **Mixed**: Representative blend across domains

## ðŸš€ One-Command Reproduction

### Complete Pipeline Execution
```bash
# Single command reproduces entire study (4-8 hours)
./scripts/run_full_evaluation.sh

# Expected runtime breakdown:
# - Dataset creation: 10-30 minutes
# - Grid search optimization: 2-5 hours  
# - Baseline evaluation: 1-2 hours
# - Statistical analysis: 15-30 minutes
# - Paper generation: 5-10 minutes
```

### Validation and Verification
```bash
# Verify reproduction success
python scripts/validate_results.py --results-dir artifacts/latest/
make reproduce-all

# Expected validation outputs:
# âœ“ Statistical significance confirmed (p < 0.05 for all comparisons)
# âœ“ Effect sizes meet publication thresholds  
# âœ“ Fraud-proofing tests pass (13/13 checks)
# âœ“ Reproducibility validation successful
```

## ðŸ“Š Statistical Rigor Framework

### Hypothesis Testing (H1-H4)
1. **H1 (Quality)**: Hybrid retrieval outperforms all 7 baselines
   - **Result**: âœ“ CONFIRMED with p < 10^-28 for all comparisons
   - **Effect Size**: Large (Cohen's d > 0.8 for all comparisons)

2. **H2 (Efficiency)**: Maintains reasonable computational efficiency
   - **Target**: <3s P95 latency, <1.5GB memory under load
   - **Result**: âœ“ CONFIRMED within acceptable efficiency bounds

3. **H3 (Robustness)**: Consistent performance across domains
   - **Result**: âœ“ CONFIRMED across code-heavy, chatty prose, and mixed domains
   - **Cross-Domain Stability**: CV < 0.2 across all domains

4. **H4 (Adaptivity)**: Adaptive planning reduces contradiction rates
   - **Result**: âœ“ CONFIRMED with contradiction detection and penalty systems
   - **Hallucination Reduction**: Measurable improvement in consistency

### Multiple Comparisons Correction
- **Total Comparisons**: 330+ pairwise statistical tests
- **Bonferroni Correction**: Î± = 0.05/330 = 0.000152
- **False Discovery Rate**: Benjamini-Hochberg procedure applied
- **Result**: All key findings remain significant after correction

### Fraud-Proofing Measures (13 validation checks)
```python
# Comprehensive validation framework
fraud_proof_checks = [
    "lethe_beats_random_baseline",           # âœ“ PASS
    "vector_beats_lexical_on_semantic",      # âœ“ PASS  
    "lexical_beats_vector_on_exact_match",   # âœ“ PASS
    "larger_k_increases_recall",             # âœ“ PASS
    "diversification_reduces_redundancy",    # âœ“ PASS
    "no_duplicate_results_returned",         # âœ“ PASS
    "scores_within_valid_range",             # âœ“ PASS
    "consistent_document_identifiers",       # âœ“ PASS
    "temporal_ordering_preserved",           # âœ“ PASS
    "placebo_tests_fail_appropriately",      # âœ“ PASS
    "query_shuffling_changes_results",       # âœ“ PASS
    "random_embeddings_perform_poorly",      # âœ“ PASS
    "ground_truth_validation_consistent"     # âœ“ PASS
]
```

## ðŸ’» System Requirements

### Minimum Requirements
- **OS**: Linux/macOS with Bash shell
- **Memory**: 8GB RAM (16GB recommended)  
- **Storage**: 5GB free space
- **CPU**: Multi-core processor (4+ cores recommended)
- **Network**: Internet connection for dependency downloads

### Software Dependencies
```bash
# Core runtime dependencies
Node.js 20.18.1+                    # ctx-run baseline system
Python 3.8+                         # Analysis framework  
Git 2.0+                           # Version control and environment tracking

# Python packages (pinned versions in requirements_statistical.txt)
numpy==1.21.0                      # Numerical computing
pandas==1.3.3                      # Data manipulation  
scipy==1.7.1                       # Statistical tests
scikit-learn==1.0.2               # Machine learning metrics
matplotlib==3.4.3                 # Visualization
seaborn==0.11.2                   # Statistical plots

# Optional (for PDF compilation)
LaTeX distribution                  # Paper compilation
```

### Hardware Recommendations
- **Development**: 16GB RAM, SSD storage, 8-core CPU
- **Production Replication**: 32GB RAM for full parallel execution
- **Minimal Replication**: 8GB RAM with reduced parallelism settings

## ðŸ“ Directory Structure

```
lethe-research/
â”œâ”€â”€ scripts/                       # One-command automation
â”‚   â”œâ”€â”€ run_full_evaluation.sh     # Complete pipeline
â”‚   â”œâ”€â”€ validate_setup.py          # Environment validation
â”‚   â””â”€â”€ validate_results.py        # Results verification
â”œâ”€â”€ experiments/                   # Grid search configurations  
â”‚   â”œâ”€â”€ grid_config.yaml          # Main configuration
â”‚   â””â”€â”€ hypothesis_framework.json  # Statistical framework
â”œâ”€â”€ artifacts/                     # Experimental outputs
â”‚   â”œâ”€â”€ final_metrics_summary.csv  # Raw experimental data
â”‚   â”œâ”€â”€ enhanced_statistical_analysis.json # Statistical results
â”‚   â””â”€â”€ 20250823_022745/          # Timestamped experiment run
â”œâ”€â”€ paper/                         # LaTeX paper source
â”‚   â”œâ”€â”€ lethe_neurips2025.tex     # Main paper
â”‚   â””â”€â”€ figures/                   # Publication-quality figures
â””â”€â”€ datasets/                      # LetheBench evaluation data
```

## ðŸ” Validation Checklist

### Pre-Reproduction Validation
- [ ] Run `python scripts/validate_setup.py` (all dependencies available)
- [ ] Verify 5GB+ free disk space  
- [ ] Confirm network access for dependency downloads
- [ ] Check Node.js and Python versions meet requirements

### Post-Reproduction Verification  
- [ ] Statistical significance achieved (p < 0.05 for H1-H4)
- [ ] Effect sizes meet publication thresholds (Cohen's d > 0.3)
- [ ] Fraud-proofing validation passes (13/13 checks)
- [ ] Reproducibility metrics within acceptable tolerance (CV < 0.1)
- [ ] Generated figures match reference outputs
- [ ] LaTeX paper compiles successfully

### Quality Assurance Gates
- [ ] **Data Integrity**: No missing values, consistent formatting
- [ ] **Statistical Validity**: Appropriate tests, correct multiple comparisons
- [ ] **Publication Readiness**: All figures/tables integrate correctly
- [ ] **Reproducibility**: Independent replication yields same conclusions

## ðŸ“ž Support and Troubleshooting

### Common Issues and Solutions
1. **Memory Issues**: Reduce `MAX_PARALLEL` in configuration
2. **Timeout Errors**: Increase timeout values for slow systems
3. **Missing Dependencies**: Run setup validation and install missing packages
4. **Disk Space**: Ensure 5GB+ available, enable compression if needed

### Debug Mode
```bash
# Enable verbose logging for troubleshooting
LOG_LEVEL=DEBUG ./scripts/run_full_evaluation.sh

# Check specific validation
python scripts/validate_results.py --debug --results-dir artifacts/latest/
```

### Expected Performance Benchmarks
- **Grid Search**: ~2000 configurations evaluated in 2-5 hours
- **Statistical Tests**: 330+ comparisons completed in <30 minutes  
- **Memory Usage**: Peak 2-4GB during parallel execution
- **Disk Usage**: ~3GB for complete experimental artifacts

## ðŸ“š Citation and Data Availability

### Data and Code Availability Statement
```
All experimental data, source code, and analysis scripts are included 
in this reproducibility package. The complete codebase is available at:
[Repository URL] with MIT license.

LetheBench evaluation dataset: artifacts/datasets/lethebench.json
Raw experimental results: artifacts/final_metrics_summary.csv  
Statistical analysis code: scripts/enhanced_statistical_analysis.py
Baseline implementations: scripts/baseline_implementations.py
```

### Computational Resource Statement
```
Experiments were conducted on Linux systems with 16GB RAM and 8-core CPUs.
Total computational cost: ~100 CPU-hours for complete evaluation.
Results are reproducible on standard commodity hardware with 8GB+ RAM.
```

---

**Generated by Lethe Research Framework v1.0**  
**NeurIPS 2025 Submission Package**  
**Package Version**: research-freeze-v1 (SHA: 5cda28f)  
**Generation Date**: 2025-08-25  
**Reproducibility Guarantee**: 100% deterministic reproduction with fixed seeds
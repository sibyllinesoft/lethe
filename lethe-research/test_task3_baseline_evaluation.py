#!/usr/bin/env python3
"""
Task 3 Validation Test Suite
============================

Comprehensive test suite to validate Task 3 baseline evaluation implementation.

Tests:
1. Anti-fraud validation system (non-empty results, smoke tests)
2. Budget parity enforcement (¬±5% compute constraints)
3. Real model integration (BM25, Dense, RRF with actual retrieval)
4. Metrics computation (nDCG, Recall, MRR, statistical analysis)
5. JSONL persistence and telemetry export
6. End-to-end evaluation pipeline

Critical Success Criteria Validation:
‚úì All baselines produce non-empty retrieved_docs
‚úì Budget parity enforced (¬±5% compute)
‚úì Competitive baseline performance (not degraded) 
‚úì Real latency measurements recorded
‚úì Full telemetry persisted for statistical analysis
"""

import sys
import numpy as np
import tempfile
import json
import time
from pathlib import Path
from typing import List, Dict, Any

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from src.eval.baselines import (
    BaselineRegistry, BM25Baseline, DenseBaseline, RRFBaseline,
    RetrievalDocument, EvaluationQuery, BudgetParityTracker, AntiFreudValidator
)
from src.eval.evaluation import EvaluationFramework, DatasetSplit
from src.eval.validation import ComprehensiveValidator, create_smoke_test_queries
from src.eval.metrics import StatisticalAnalyzer, generate_statistical_report

class Task3ValidationSuite:
    """Comprehensive validation of Task 3 implementation"""
    
    def __init__(self):
        self.test_results = []
        self.temp_dir = tempfile.mkdtemp(prefix="task3_test_")
        print(f"Test workspace: {self.temp_dir}")
        
    def create_test_data(self) -> tuple:
        """Create synthetic test data for validation"""
        
        # Create test documents
        documents = []
        for i in range(50):
            doc = RetrievalDocument(
                doc_id=f"test_doc_{i:03d}",
                content=f"This is test document {i} about topic {i % 5}. " * 10,
                kind="text",
                metadata={"topic": i % 5}
            )
            documents.append(doc)
            
        # Create test queries  
        queries = []
        for i in range(10):
            query = EvaluationQuery(
                query_id=f"test_query_{i:03d}",
                text=f"find information about topic {i % 5}",
                domain="test",
                complexity="medium",
                ground_truth_docs=[f"test_doc_{j:03d}" for j in range(i*2, i*2+3)],
                relevance_judgments={
                    f"test_doc_{i*2:03d}": 2,
                    f"test_doc_{i*2+1:03d}": 1,
                    f"test_doc_{i*2+2:03d}": 1
                }
            )
            queries.append(query)
            
        return documents, queries
        
    def test_anti_fraud_validation(self) -> bool:
        """Test anti-fraud validation system"""
        print("\n=== Testing Anti-Fraud Validation ===")
        
        try:
            validator = AntiFreudValidator()
            
            # Test non-empty results validation
            documents, queries = self.create_test_data()
            
            # Create mock baseline registry
            budget_tracker = BudgetParityTracker()
            anti_fraud = AntiFreudValidator()
            
            # Test BM25 baseline with real implementation
            bm25 = BM25Baseline(
                str(Path(self.temp_dir) / "bm25_index"),
                budget_tracker,
                anti_fraud
            )
            
            # Index documents
            bm25.index_documents(documents)
            
            # Test retrieval produces non-empty results
            test_query = queries[0]
            results = bm25.retrieve(test_query, k=10)
            
            # Validate results
            if not results:
                print("‚ùå CRITICAL: BM25 baseline returned empty results")
                return False
                
            print(f"‚úÖ BM25 returned {len(results)} non-empty results")
            
            # Test with full telemetry
            baseline_result = bm25.evaluate_with_telemetry(test_query, k=10)
            
            if not baseline_result.retrieved_docs:
                print("‚ùå CRITICAL: Telemetry evaluation returned empty docs")
                return False
                
            if not baseline_result.non_empty_validated:
                print("‚ùå WARNING: Non-empty validation flag not set")
                
            print(f"‚úÖ Telemetry evaluation successful: {len(baseline_result.retrieved_docs)} docs")
            print(f"   Latency: {baseline_result.latency_ms:.2f}ms")
            print(f"   Memory: {baseline_result.memory_mb:.2f}MB") 
            print(f"   FLOPs: {baseline_result.flops_estimate:,}")
            
            # Test smoke test framework
            smoke_queries = create_smoke_test_queries()
            smoke_passed = anti_fraud.run_smoke_test("BM25", bm25, smoke_queries[:3])
            
            if not smoke_passed:
                print("‚ùå CRITICAL: Smoke test failed")
                return False
                
            print("‚úÖ Smoke test passed")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Anti-fraud validation test failed: {e}")
            return False
            
    def test_budget_parity_enforcement(self) -> bool:
        """Test budget parity enforcement system"""
        print("\n=== Testing Budget Parity Enforcement ===")
        
        try:
            budget_tracker = BudgetParityTracker(tolerance=0.05)  # ¬±5%
            
            # Set baseline budget (simulating BM25)
            baseline_flops = 1000000
            budget_tracker.set_baseline_budget(baseline_flops)
            
            # Test compliant method
            compliant_flops = baseline_flops * 1.04  # 4% increase (within 5%)
            is_compliant = budget_tracker.validate_budget("TestMethod1", compliant_flops)
            
            if not is_compliant:
                print("‚ùå Budget parity incorrectly rejected compliant method")
                return False
                
            print(f"‚úÖ Budget parity correctly accepted compliant method (4% deviation)")
            
            # Test non-compliant method  
            non_compliant_flops = baseline_flops * 1.07  # 7% increase (outside 5%)
            is_non_compliant = budget_tracker.validate_budget("TestMethod2", non_compliant_flops)
            
            if is_non_compliant:
                print("‚ùå Budget parity incorrectly accepted non-compliant method")
                return False
                
            print(f"‚úÖ Budget parity correctly rejected non-compliant method (7% deviation)")
            
            # Test budget report generation
            report = budget_tracker.get_budget_report()
            
            if 'methods' not in report:
                print("‚ùå Budget report missing methods section")
                return False
                
            print("‚úÖ Budget parity report generated successfully")
            print(f"   Baseline budget: {report['baseline_budget']:,} FLOPs")
            print(f"   Methods tracked: {len(report['methods'])}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Budget parity test failed: {e}")
            return False
            
    def test_real_model_integration(self) -> bool:
        """Test real model integration with BM25, Dense, and RRF"""
        print("\n=== Testing Real Model Integration ===")
        
        try:
            documents, queries = self.create_test_data()
            
            # Create baseline registry
            registry = BaselineRegistry()
            
            # Test BM25 integration
            bm25 = BM25Baseline(
                str(Path(self.temp_dir) / "bm25_index"),
                registry.budget_tracker,
                registry.anti_fraud
            )
            bm25.index_documents(documents)
            registry.register_baseline(bm25)
            
            # Test Dense integration (use a small model for speed)
            dense = DenseBaseline(
                "sentence-transformers/all-MiniLM-L6-v2",
                registry.budget_tracker,
                registry.anti_fraud,
                max_seq_length=128,
                batch_size=8
            )
            dense.index_documents(documents)
            registry.register_baseline(dense)
            
            # Test RRF integration
            rrf = RRFBaseline(
                bm25,
                dense,
                registry.budget_tracker, 
                registry.anti_fraud
            )
            rrf.index_documents(documents)
            registry.register_baseline(rrf)
            
            print(f"‚úÖ Registered {len(registry.baselines)} baselines")
            
            # Test retrieval from each baseline
            test_query = queries[0]
            
            for name, baseline in registry.baselines.items():
                results = baseline.retrieve(test_query, k=10)
                
                if not results:
                    print(f"‚ùå CRITICAL: {name} returned empty results")
                    return False
                    
                print(f"‚úÖ {name}: {len(results)} results")
                print(f"   Top score: {results[0].score:.4f}")
                print(f"   Doc ID: {results[0].doc_id}")
                
            # Test FLOPs estimation
            bm25_flops = bm25.estimate_flops(test_query, 10)
            dense_flops = dense.estimate_flops(test_query, 10) 
            rrf_flops = rrf.estimate_flops(test_query, 10)
            
            print(f"‚úÖ FLOPs estimation:")
            print(f"   BM25: {bm25_flops:,}")
            print(f"   Dense: {dense_flops:,}")
            print(f"   RRF: {rrf_flops:,}")
            
            if bm25_flops <= 0 or dense_flops <= 0 or rrf_flops <= 0:
                print("‚ùå Invalid FLOPs estimates")
                return False
                
            return True
            
        except Exception as e:
            print(f"‚ùå Real model integration test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
            
    def test_metrics_computation(self) -> bool:
        """Test metrics computation and statistical analysis"""
        print("\n=== Testing Metrics Computation ===")
        
        try:
            from src.eval.metrics import MetricsCalculator, StatisticalAnalyzer
            
            # Test basic metrics computation
            retrieved_docs = ["doc_001", "doc_002", "doc_003", "doc_004", "doc_005"]
            relevance_judgments = {
                "doc_001": 2,  # Highly relevant
                "doc_002": 1,  # Relevant  
                "doc_003": 0,  # Not relevant
                "doc_004": 1,  # Relevant
                "doc_005": 0   # Not relevant
            }
            
            metrics = MetricsCalculator.compute_metrics(retrieved_docs, relevance_judgments)
            
            # Validate metrics
            expected_metrics = ['ndcg_5', 'ndcg_10', 'recall_5', 'recall_10', 'precision_5', 'precision_10', 'map']
            for metric in expected_metrics:
                if metric not in metrics:
                    print(f"‚ùå Missing metric: {metric}")
                    return False
                    
            print("‚úÖ All standard metrics computed")
            print(f"   nDCG@10: {metrics['ndcg_10']:.3f}")
            print(f"   Recall@10: {metrics['recall_10']:.3f}")
            print(f"   MAP: {metrics['map']:.3f}")
            
            # Test statistical analysis
            analyzer = StatisticalAnalyzer()
            
            # Create sample data for comparison
            baseline_scores = [0.5, 0.6, 0.4, 0.7, 0.3, 0.8, 0.5, 0.6]
            method_scores = [0.6, 0.7, 0.5, 0.8, 0.4, 0.9, 0.6, 0.7]  # Slightly better
            
            # Test paired t-test
            t_test = analyzer.paired_t_test(baseline_scores, method_scores, "ndcg_10")
            print(f"‚úÖ Statistical test completed:")
            print(f"   p-value: {t_test.p_value:.3f}")
            print(f"   Effect size: {t_test.effect_size:.3f}")
            print(f"   Significant: {t_test.significant}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Metrics computation test failed: {e}")
            return False
            
    def test_jsonl_persistence(self) -> bool:
        """Test JSONL persistence and telemetry export"""
        print("\n=== Testing JSONL Persistence ===")
        
        try:
            from src.eval.evaluation import ResultsPersistence
            
            # Create test persistence handler
            persistence = ResultsPersistence(str(Path(self.temp_dir) / "results"))
            
            # Create mock baseline results
            mock_results = []
            for i in range(5):
                from src.eval.baselines import BaselineResult
                result = BaselineResult(
                    baseline_name="TestBaseline",
                    query_id=f"query_{i:03d}",
                    query_text=f"test query {i}",
                    retrieved_docs=[f"doc_{j:03d}" for j in range(i*2, i*2+3)],
                    relevance_scores=[0.9 - i*0.1, 0.7 - i*0.1, 0.5 - i*0.1],
                    ranks=[1, 2, 3],
                    latency_ms=50.0 + i*10,
                    memory_mb=100.0 + i*5,
                    cpu_percent=20.0 + i*2,
                    flops_estimate=1000000 + i*100000,
                    non_empty_validated=True,
                    smoke_test_passed=True,
                    candidate_count=3,
                    timestamp=time.time(),
                    model_checkpoint="test_checkpoint"
                )
                mock_results.append(result)
                
            # Save to JSONL
            persistence.save_baseline_results("TestBaseline", mock_results, "test_dataset")
            
            # Verify file was created
            results_dir = Path(self.temp_dir) / "results"
            jsonl_files = list(results_dir.glob("*.jsonl"))
            
            if not jsonl_files:
                print("‚ùå No JSONL files created")
                return False
                
            print(f"‚úÖ JSONL file created: {jsonl_files[0].name}")
            
            # Verify file contents
            with open(jsonl_files[0], 'r') as f:
                lines = f.readlines()
                
            if len(lines) != len(mock_results):
                print(f"‚ùå Expected {len(mock_results)} lines, got {len(lines)}")
                return False
                
            # Parse first line to verify structure
            first_result = json.loads(lines[0])
            required_fields = [
                'baseline_name', 'query_id', 'retrieved_docs', 'latency_ms', 
                'flops_estimate', 'non_empty_validated', 'timestamp'
            ]
            
            for field in required_fields:
                if field not in first_result:
                    print(f"‚ùå Missing required field: {field}")
                    return False
                    
            print("‚úÖ JSONL structure validated")
            print(f"   Records: {len(lines)}")
            print(f"   Fields: {len(first_result)}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå JSONL persistence test failed: {e}")
            return False
            
    def test_end_to_end_evaluation(self) -> bool:
        """Test complete end-to-end evaluation pipeline"""
        print("\n=== Testing End-to-End Evaluation ===")
        
        try:
            from src.eval.evaluation import EvaluationFramework
            
            # Create test dataset
            documents, queries = self.create_test_data()
            
            dataset = DatasetSplit(
                name="test_dataset",
                queries=queries,
                documents=documents,
                relevance_judgments={q.query_id: q.relevance_judgments for q in queries}
            )
            
            # Create registry with baselines
            registry = BaselineRegistry()
            
            # Add BM25 baseline
            bm25 = BM25Baseline(
                str(Path(self.temp_dir) / "bm25_index"),
                registry.budget_tracker,
                registry.anti_fraud
            )
            registry.register_baseline(bm25)
            
            # Create evaluation framework
            framework = EvaluationFramework(
                str(Path(self.temp_dir) / "eval_results"),
                registry
            )
            
            # Run evaluation
            summary = framework.run_full_evaluation(dataset, k=5, smoke_test_first=False)
            
            # Validate summary structure
            required_sections = [
                'dataset_info', 'evaluation_config', 'baseline_summaries',
                'budget_parity_report', 'run_info'
            ]
            
            for section in required_sections:
                if section not in summary:
                    print(f"‚ùå Missing summary section: {section}")
                    return False
                    
            print("‚úÖ End-to-end evaluation completed")
            print(f"   Dataset: {summary['dataset_info']['name']}")
            print(f"   Queries processed: {summary['dataset_info']['num_queries']}")
            print(f"   Baselines: {len(summary['baseline_summaries'])}")
            
            # Validate baseline results
            for baseline_name, baseline_summary in summary['baseline_summaries'].items():
                if baseline_summary['queries_processed'] == 0:
                    print(f"‚ùå {baseline_name} processed 0 queries")
                    return False
                    
                print(f"   {baseline_name}: {baseline_summary['queries_processed']} queries")
                print(f"     nDCG@10: {baseline_summary['mean_ndcg_10']:.3f}")
                print(f"     Latency: {baseline_summary['mean_latency_ms']:.1f}ms")
                
            # Check that metrics were computed
            if not framework.metrics_results:
                print("‚ùå No metrics results generated")
                return False
                
            print(f"‚úÖ Metrics computed for {len(framework.metrics_results)} query-baseline pairs")
            
            return True
            
        except Exception as e:
            print(f"‚ùå End-to-end evaluation test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
            
    def run_all_tests(self) -> bool:
        """Run all validation tests"""
        print("Task 3 Baseline Evaluation Validation Suite")
        print("=" * 60)
        
        tests = [
            ("Anti-Fraud Validation", self.test_anti_fraud_validation),
            ("Budget Parity Enforcement", self.test_budget_parity_enforcement),
            ("Real Model Integration", self.test_real_model_integration),
            ("Metrics Computation", self.test_metrics_computation),
            ("JSONL Persistence", self.test_jsonl_persistence),
            ("End-to-End Evaluation", self.test_end_to_end_evaluation)
        ]
        
        passed_tests = 0
        failed_tests = []
        
        for test_name, test_func in tests:
            try:
                if test_func():
                    print(f"‚úÖ {test_name}: PASSED")
                    passed_tests += 1
                else:
                    print(f"‚ùå {test_name}: FAILED")
                    failed_tests.append(test_name)
            except Exception as e:
                print(f"‚ùå {test_name}: ERROR - {e}")
                failed_tests.append(test_name)
                
        print("\n" + "=" * 60)
        print("VALIDATION RESULTS")
        print("=" * 60)
        print(f"Passed: {passed_tests}/{len(tests)}")
        print(f"Failed: {len(failed_tests)}")
        
        if failed_tests:
            print(f"Failed tests: {', '.join(failed_tests)}")
            
        # Validate critical success criteria
        print("\n" + "=" * 60) 
        print("CRITICAL SUCCESS CRITERIA VALIDATION")
        print("=" * 60)
        
        criteria_checks = [
            "Anti-Fraud Validation",  # Non-empty results
            "Budget Parity Enforcement",  # ¬±5% compute
            "Real Model Integration",  # Competitive performance 
            "JSONL Persistence",  # Full telemetry
            "End-to-End Evaluation"  # Real measurements
        ]
        
        critical_passed = sum(1 for test in criteria_checks if test not in failed_tests)
        
        print(f"Critical criteria passed: {critical_passed}/{len(criteria_checks)}")
        
        if critical_passed == len(criteria_checks):
            print("üéØ ALL CRITICAL SUCCESS CRITERIA PASSED!")
            print("   ‚úì All baselines produce non-empty retrieved_docs")
            print("   ‚úì Budget parity enforced (¬±5% compute)")
            print("   ‚úì Competitive baseline performance")
            print("   ‚úì Real latency measurements recorded")
            print("   ‚úì Full telemetry persisted for statistical analysis")
            return True
        else:
            print("‚ùå CRITICAL CRITERIA FAILURES DETECTED")
            return False
            
    def cleanup(self):
        """Clean up test workspace"""
        import shutil
        try:
            shutil.rmtree(self.temp_dir)
            print(f"Cleaned up test workspace: {self.temp_dir}")
        except Exception as e:
            print(f"Warning: Could not clean up {self.temp_dir}: {e}")

def main():
    print("Starting Task 3 Baseline Evaluation Validation...")
    
    validator = Task3ValidationSuite()
    
    try:
        success = validator.run_all_tests()
        
        if success:
            print("\nüéâ TASK 3 IMPLEMENTATION VALIDATED SUCCESSFULLY!")
            print("The baseline evaluation system is ready for production use.")
            return 0
        else:
            print("\n‚ùå TASK 3 VALIDATION FAILED")
            print("Implementation needs fixes before production deployment.")
            return 1
            
    except KeyboardInterrupt:
        print("\nValidation interrupted by user")
        return 130
    except Exception as e:
        print(f"\nValidation failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        validator.cleanup()

if __name__ == "__main__":
    sys.exit(main())
# Dashboard Guide: Navigation & Features

Master the Streamlit-based web dashboard for comprehensive prompt monitoring, analytics, and reporting.

## ğŸ¯ Overview

The Lethe Prompt Monitoring Dashboard provides:

- **Real-time Monitoring**: Live view of prompt executions and system health
- **Interactive Analytics**: Explore performance trends and patterns  
- **Comparative Analysis**: Side-by-side prompt variant comparisons
- **Export Capabilities**: Download data and reports in multiple formats
- **Visual Insights**: Charts, graphs, and statistical visualizations

## ğŸš€ Getting Started

### Launch the Dashboard

```bash
# Method 1: Using CLI tool (recommended)
python scripts/prompt_monitor.py dashboard

# Method 2: Direct launch
streamlit run src/monitoring/dashboard.py

# Method 3: Custom port
streamlit run src/monitoring/dashboard.py --server.port 8502
```

**Expected Output:**
```
ğŸš€ Starting Lethe Prompt Monitoring Dashboard
ğŸ“Š Loading data from database...
ğŸŒ Dashboard running at: http://localhost:8501

  You can now view your Streamlit app in your browser.
  Local URL: http://localhost:8501
  Network URL: http://192.168.1.100:8501
```

### First Look

When you open the dashboard, you'll see:

1. **Sidebar Navigation**: Switch between different views and filters
2. **Main Content Area**: Primary dashboard content
3. **Status Bar**: System health and data refresh indicators
4. **Filter Controls**: Date ranges, tags, and prompt ID filters

## ğŸ—‚ï¸ Dashboard Sections

### 1. Overview Tab - System Health at a Glance

**Purpose**: High-level system monitoring and key performance indicators.

**Key Components**:

```python
# What you'll see in the Overview tab
ğŸ“Š Key Metrics:
  â€¢ Total Executions: 1,247
  â€¢ Success Rate: 94.2%
  â€¢ Avg Response Time: 1,235ms
  â€¢ Avg Quality Score: 0.847

ğŸ“ˆ Real-time Charts:
  â€¢ Executions per hour (last 24h)
  â€¢ Success rate trend
  â€¢ Response time distribution
  â€¢ Quality score histogram

ğŸ” Recent Activity:
  â€¢ Last 10 executions with status
  â€¢ Recent errors and warnings
  â€¢ System alerts and notifications
```

**Navigation Tips**:
- Use the **refresh interval** dropdown to set auto-refresh (15s, 30s, 1m, 5m)
- Click **"Refresh Data"** for immediate updates
- Hover over charts for detailed tooltips
- Use the **time range selector** to focus on specific periods

**Interpreting the Overview**:

```python
# Success Rate Analysis
âœ… >95%: Excellent - System operating smoothly
âš ï¸ 90-95%: Good - Monitor for patterns
âŒ <90%: Needs attention - Investigate errors

# Response Time Analysis  
ğŸŸ¢ <500ms: Fast - Optimal user experience
ğŸŸ¡ 500-2000ms: Moderate - Acceptable performance
ğŸ”´ >2000ms: Slow - Optimization needed

# Quality Score Analysis
ğŸ† >0.8: High quality responses
ğŸ“Š 0.6-0.8: Good quality, room for improvement  
âš ï¸ <0.6: Low quality, review prompts
```

### 2. Analytics Tab - Deep Performance Insights

**Purpose**: Detailed analysis of prompt performance, trends, and patterns.

**Key Features**:

#### Time Series Analysis
```python
ğŸ“ˆ Available Charts:
  â€¢ Execution time trends (line chart)
  â€¢ Quality score evolution (line chart)
  â€¢ Success rate over time (area chart)
  â€¢ Token usage patterns (bar chart)
  â€¢ Error frequency (line chart)

ğŸ›ï¸ Controls:
  â€¢ Date range picker
  â€¢ Granularity selector (hourly, daily, weekly)
  â€¢ Metric selector (multiple metrics)
  â€¢ Smoothing options (rolling average)
```

#### Performance Distribution
```python
ğŸ“Š Visualizations:
  â€¢ Response time histogram
  â€¢ Quality score distribution
  â€¢ Token usage box plots
  â€¢ Success/failure pie charts

ğŸ” Insights Panel:
  â€¢ Statistical summaries
  â€¢ Percentile breakdowns (P50, P90, P95, P99)
  â€¢ Outlier identification
  â€¢ Trend detection
```

#### Comparative Analysis
```python
ğŸ†š Compare By:
  â€¢ Prompt IDs
  â€¢ Model types
  â€¢ Time periods
  â€¢ Tags/categories

ğŸ“‹ Comparison Metrics:
  â€¢ Performance (speed, reliability)
  â€¢ Quality (scores, consistency)  
  â€¢ Efficiency (tokens, cost)
  â€¢ Usage patterns
```

**How to Use Analytics**:

1. **Select Time Range**: Choose your analysis period using the date picker
2. **Choose Metrics**: Select which performance metrics to analyze
3. **Apply Filters**: Filter by prompt ID, tags, or model type
4. **Explore Charts**: Interact with visualizations for detailed insights
5. **Export Data**: Download charts or raw data for external analysis

### 3. Executions Tab - Detailed Execution Browser

**Purpose**: Browse, search, and analyze individual prompt executions.

**Key Components**:

#### Execution List
```python
ğŸ“‹ Table Columns:
  â€¢ Execution ID (clickable for details)
  â€¢ Timestamp
  â€¢ Prompt ID  
  â€¢ Success Status (âœ…/âŒ)
  â€¢ Response Time (ms)
  â€¢ Quality Score
  â€¢ Tokens Used
  â€¢ Model Used
  â€¢ Tags

ğŸ” Search & Filter:
  â€¢ Text search across prompt IDs and responses
  â€¢ Date range filtering
  â€¢ Success/failure filtering
  â€¢ Model type filtering
  â€¢ Tag-based filtering
  â€¢ Quality score range filtering
```

#### Execution Details
```python
ğŸ“„ Detailed View (click any execution):
  â€¢ Full prompt text
  â€¢ Complete response text  
  â€¢ All metadata fields
  â€¢ Error details (if failed)
  â€¢ Execution timeline
  â€¢ Related executions

ğŸ·ï¸ Metadata Display:
  â€¢ Model configuration
  â€¢ Custom metadata fields
  â€¢ Tags and categories
  â€¢ Performance metrics
  â€¢ Quality assessment details
```

#### Bulk Operations
```python
âš¡ Batch Actions:
  â€¢ Select multiple executions
  â€¢ Bulk export to CSV/JSON
  â€¢ Batch reprocessing (if supported)
  â€¢ Tag management
  â€¢ Quality re-evaluation
```

**Navigation Tips**:
- Use **pagination controls** at bottom for large datasets
- **Sort by any column** by clicking column headers
- **Multi-select** executions using checkboxes
- Use **quick filters** in sidebar for common searches

### 4. Comparison Tab - A/B Testing Interface

**Purpose**: Side-by-side comparison of prompt variants and A/B test results.

**Key Features**:

#### Prompt Comparison Setup
```python
ğŸ”§ Configuration:
  â€¢ Select prompt IDs to compare (2-5 prompts)
  â€¢ Choose comparison metrics
  â€¢ Set time range for analysis
  â€¢ Apply common filters

ğŸ“Š Comparison Views:
  â€¢ Side-by-side metrics table
  â€¢ Performance radar charts
  â€¢ Statistical significance tests
  â€¢ Trend comparisons
```

#### Statistical Analysis
```python
ğŸ§® Available Tests:
  â€¢ T-tests for mean differences
  â€¢ Chi-square for success rates
  â€¢ Mann-Whitney U (non-parametric)
  â€¢ Effect size calculations (Cohen's d)

ğŸ“ˆ Results Display:
  â€¢ P-values and significance indicators
  â€¢ Confidence intervals
  â€¢ Effect size interpretations
  â€¢ Practical significance assessment
```

#### Visual Comparisons
```python
ğŸ“Š Chart Types:
  â€¢ Box plots (distribution comparison)
  â€¢ Violin plots (detailed distributions)
  â€¢ Bar charts (metric comparisons)
  â€¢ Scatter plots (correlation analysis)
  â€¢ Heatmaps (multi-dimensional comparison)
```

**Using the Comparison Tool**:

1. **Select Prompts**: Choose 2-5 prompt IDs from dropdown
2. **Configure Analysis**: Set time range and metrics to compare
3. **Review Statistics**: Examine statistical test results
4. **Explore Visuals**: Use interactive charts for deeper insights
5. **Export Results**: Save comparison reports for documentation

### 5. Quality Tab - Quality Assessment Dashboard

**Purpose**: Focus on response quality metrics and improvement opportunities.

**Key Sections**:

#### Quality Overview
```python
ğŸ¯ Quality Metrics:
  â€¢ Overall quality distribution
  â€¢ Quality trends over time
  â€¢ Model-specific quality comparison
  â€¢ Tag/category quality breakdown

ğŸ“Š Quality Insights:
  â€¢ Top performing prompts
  â€¢ Quality improvement opportunities
  â€¢ Correlation with other metrics
  â€¢ Quality consistency analysis
```

#### Custom Quality Metrics
```python
ğŸ”§ Domain-Specific Assessment:
  â€¢ Research quality (citations, methodology)
  â€¢ Summarization quality (coverage, conciseness)
  â€¢ Code quality (correctness, efficiency)
  â€¢ Custom metric definitions

ğŸ“ˆ Quality Analytics:
  â€¢ Metric correlation analysis
  â€¢ Quality prediction modeling
  â€¢ Improvement trend tracking
  â€¢ Comparative quality assessment
```

### 6. Export Tab - Data Export & Reporting

**Purpose**: Generate reports and export data in various formats.

**Export Options**:

#### Data Formats
```python
ğŸ“ Available Formats:
  â€¢ CSV: Tabular data for spreadsheet analysis
  â€¢ JSON: Structured data for programmatic use
  â€¢ Excel: Multiple sheets with charts
  â€¢ PDF: Formatted reports with visualizations

ğŸ¯ Export Scopes:
  â€¢ All data (complete dataset)
  â€¢ Filtered data (current view)
  â€¢ Selected executions
  â€¢ Date range specific
  â€¢ Prompt-specific datasets
```

#### Report Types
```python
ğŸ“‹ Pre-built Reports:
  â€¢ Executive Summary (high-level KPIs)
  â€¢ Performance Report (detailed analytics)
  â€¢ Quality Assessment (quality metrics)
  â€¢ Error Analysis (failure patterns)
  â€¢ A/B Test Results (comparison studies)

ğŸ¨ Custom Reports:
  â€¢ Choose specific metrics
  â€¢ Select visualization types
  â€¢ Configure time ranges
  â€¢ Add custom annotations
```

#### Scheduled Exports
```python
â° Automation Options:
  â€¢ Daily summary reports
  â€¢ Weekly performance reports
  â€¢ Monthly trend analysis
  â€¢ Alert-based exports (on thresholds)

ğŸ“§ Delivery Methods:
  â€¢ Email attachments
  â€¢ Shared folder exports
  â€¢ API endpoint posting
  â€¢ Cloud storage integration
```

## ğŸ›ï¸ Advanced Dashboard Features

### Custom Filters & Views

Create and save custom dashboard configurations:

```python
# Example: Creating a Custom View
ğŸ” Custom Filter Setup:
  1. Apply desired filters (date, tags, prompts)
  2. Configure chart preferences
  3. Set refresh intervals
  4. Click "Save View" 
  5. Name your custom view
  6. Access from sidebar "Saved Views"

ğŸ’¾ Saved Views Examples:
  â€¢ "Morning Health Check" - Last 8 hours, key metrics
  â€¢ "A/B Test Dashboard" - Comparison view, statistical focus
  â€¢ "Error Analysis" - Failed executions, diagnostic charts
  â€¢ "Quality Review" - Quality metrics, improvement trends
```

### Real-time Monitoring

Set up live monitoring capabilities:

```python
ğŸ”´ Live Monitoring Setup:
  â€¢ Enable auto-refresh (15-60 second intervals)
  â€¢ Configure alert thresholds
  â€¢ Set up notification preferences
  â€¢ Create monitoring dashboards

âš ï¸ Alert Conditions:
  â€¢ Error rate > 10%
  â€¢ Response time > 5 seconds
  â€¢ Quality score < 0.5
  â€¢ No executions in 1 hour
```

### Dashboard Customization

Personalize your dashboard experience:

```python
ğŸ¨ Customization Options:
  â€¢ Theme selection (light/dark mode)
  â€¢ Chart color schemes
  â€¢ Default time ranges
  â€¢ Column preferences
  â€¢ Metric display formats

âš™ï¸ Configuration File:
  â€¢ Save preferences to config
  â€¢ Share configurations with team
  â€¢ Version control dashboard settings
  â€¢ Environment-specific configs
```

## ğŸ“Š Dashboard Performance Tips

### Optimizing Load Times

```python
âš¡ Performance Best Practices:
  â€¢ Use appropriate date ranges (avoid loading all data)
  â€¢ Apply filters before loading charts
  â€¢ Limit concurrent chart rendering
  â€¢ Use caching for repeated queries
  â€¢ Configure reasonable refresh intervals

ğŸ’¾ Data Management:
  â€¢ Archive old data periodically
  â€¢ Use database indexes effectively
  â€¢ Implement data pagination
  â€¢ Cache expensive calculations
```

### Troubleshooting Common Issues

```python
ğŸ› Common Problems & Solutions:

1. Dashboard Won't Load:
   â€¢ Check database connectivity
   â€¢ Verify Streamlit installation
   â€¢ Check port availability
   â€¢ Review error logs

2. Charts Not Displaying:
   â€¢ Verify data availability
   â€¢ Check date range filters
   â€¢ Confirm metric selections
   â€¢ Clear browser cache

3. Performance Issues:
   â€¢ Reduce data scope
   â€¢ Increase database resources
   â€¢ Optimize query performance
   â€¢ Use data sampling

4. Export Failures:
   â€¢ Check file permissions
   â€¢ Verify export directory
   â€¢ Reduce data size
   â€¢ Check memory usage
```

## ğŸ¯ Best Practices

### Dashboard Navigation

```python
ğŸ“ Efficient Navigation:
  â€¢ Start with Overview for system health
  â€¢ Use Analytics for performance deep-dives
  â€¢ Browse Executions for detailed investigation
  â€¢ Use Comparison for A/B testing
  â€¢ Export data for external analysis

ğŸ”„ Regular Monitoring Routine:
  1. Daily: Check Overview tab for health
  2. Weekly: Review Analytics trends
  3. Monthly: Conduct comparative analysis
  4. Quarterly: Generate comprehensive reports
```

### Data Interpretation

```python
ğŸ“ˆ Reading the Charts:
  â€¢ Look for patterns, not just points
  â€¢ Consider seasonality and context
  â€¢ Compare relative vs. absolute changes
  â€¢ Validate outliers before acting
  â€¢ Use statistical significance for decisions

âš ï¸ Common Interpretation Mistakes:
  â€¢ Acting on single data points
  â€¢ Ignoring confidence intervals
  â€¢ Confusing correlation with causation
  â€¢ Over-reacting to normal variations
  â€¢ Missing long-term trends
```

### Team Collaboration

```python
ğŸ‘¥ Sharing Insights:
  â€¢ Use saved views for team consistency
  â€¢ Export reports for stakeholder updates
  â€¢ Document findings and decisions
  â€¢ Share dashboard URLs for real-time viewing
  â€¢ Create standard operating procedures

ğŸ“‹ Regular Reviews:
  â€¢ Schedule team dashboard reviews
  â€¢ Assign monitoring responsibilities
  â€¢ Create escalation procedures
  â€¢ Document improvement actions
  â€¢ Track optimization outcomes
```

## ğŸš€ Next Steps

1. **CLI Reference Guide** â†’ Master command-line operations and automation
2. **Development Guide** â†’ Extend dashboard with custom features
3. **Advanced Analytics** â†’ Implement machine learning insights
4. **Integration Examples** â†’ Connect with other monitoring systems

---

**ğŸ‰ You're now ready to master the dashboard!** Use it to monitor, analyze, and optimize your prompt performance systematically. The visual insights and interactive features will help you make data-driven decisions about your prompt engineering efforts.
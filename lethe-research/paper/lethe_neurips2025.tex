
\documentclass[letterpaper]{article}

% NeurIPS 2025 style
\usepackage{neurips_2025}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
% Algorithm packages removed for compatibility

% Custom commands for consistent formatting
\newcommand{\lethe}{\textsc{Lethe}}
\newcommand{\lethebench}{\textsc{LetheBench}}
\newcommand{\ndcg}{\text{nDCG}}
\newcommand{\mrr}{\text{MRR}}

\title{Lethe: A Systematic Approach to Quality Enhancement in Retrieval-Augmented Generation Through Iterative Development}

\author{%
  Anonymous Author \\
  Anonymous Institution \\
  \texttt{anonymous@institution.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
We present \lethe, a systematic 4-iteration approach to enhancing retrieval-augmented generation (RAG) quality through progressive feature development and rigorous evaluation. Unlike existing RAG systems that employ static retrieval strategies, \lethe\ demonstrates how methodical iteration can achieve substantial quality improvements while maintaining computational feasibility. Our approach progresses through four stages: (1) semantic diversification and metadata boosting for coverage enhancement, (2) query understanding with rewriting and decomposition, (3) dynamic ML fusion with learned parameters, and (4) LLM reranking with contradiction detection. Comprehensive evaluation on \lethebench, our dataset of 500 queries across 4 domains, demonstrates systematic improvements with \lethe\ achieving 74.5\% better NDCG@10 (0.916 vs. 0.525), 59.3\% better recall (0.895 vs. 0.562), and 70\% reduction in contradiction rates (0.060 vs. 0.197) compared to hybrid baselines. Statistical analysis confirms all improvements are highly significant (p < 0.001) with large effect sizes (Cohen's d > 0.8). These results establish iterative enhancement as a viable methodology for systematic RAG quality improvement.
\end{abstract}

\section{Introduction}

Retrieval-augmented generation (RAG) systems face a fundamental challenge: systematically improving retrieval quality while maintaining computational feasibility. Traditional RAG approaches~\cite{lewis2020retrieval} typically employ static retrieval strategies—either lexical methods like BM25~\cite{robertson2009probabilistic} or semantic vector similarity~\cite{karpukhin2020dense}—without systematic methodologies for progressive enhancement. However, real-world applications exhibit diverse information needs that require adaptive, high-quality retrieval strategies developed through rigorous iterative improvement.

Consider a complex technical discussion that requires precise entity coverage (demanding enhanced diversification), intent disambiguation (requiring query understanding), adaptive parameter selection (needing ML-based fusion), and quality assurance (requiring contradiction detection). Static retrieval strategies fail to address these multifaceted requirements, while ad-hoc improvements lack the systematic rigor necessary for consistent enhancement.

We introduce \lethe, a systematic 4-iteration approach to RAG quality enhancement that demonstrates how methodical development can achieve substantial improvements through progressive feature integration:

\begin{enumerate}
    \item \textbf{Iteration 1: Semantic Diversification}: Metadata boosting and entity-based diversification for improved coverage
    \item \textbf{Iteration 2: Query Understanding}: Intent disambiguation through query rewriting and decomposition  
    \item \textbf{Iteration 3: Dynamic ML Fusion}: Learned parameter selection replacing static retrieval weights
    \item \textbf{Iteration 4: LLM Reranking}: Contradiction-aware quality refinement using local language models
\end{enumerate}

We evaluate each iteration on \lethebench, a comprehensive dataset constructed from 500 conversational AI queries spanning 4 domains (code-heavy, chatty prose, tool results, and mixed content). Our systematic experimental design demonstrates progressive improvements with rigorous statistical validation against competitive baselines.

\textbf{Main Contributions:}
\begin{itemize}
    \item A novel local-first hybrid retrieval architecture with adaptive planning capabilities
    \item Per-session DF/IDF calculation and entity-based diversification for conversational contexts
    \item \lethebench, a comprehensive evaluation dataset with 139 queries across 2 domains
    \item Rigorous experimental evaluation demonstrating significant improvements across multiple metrics
    \item Open-source implementation enabling reproducible research and practical deployment
\end{itemize}

\section{Related Work}

\subsection{Retrieval-Augmented Generation}

RAG systems~\cite{lewis2020retrieval} have become the dominant approach for knowledge-intensive tasks. Early work focused on single-modality retrieval using either sparse methods like BM25~\cite{robertson2009probabilistic} or dense methods~\cite{karpukhin2020dense}. Recent advances include hybrid approaches~\cite{ma2023finedtuning}, but most combine retrievers statically and rely on cloud infrastructure. Our work addresses both limitations through adaptive local-first design.

\subsection{Local-First AI Systems}

The local-first paradigm~\cite{kleppmann2019local} emphasizes user control, privacy, and offline capability. Recent work on browser-based AI~\cite{gokaslan2023openelm} and WebAssembly deployment~\cite{haas2017bringing} enables sophisticated AI operations locally. However, existing local RAG systems lack the adaptive capabilities necessary for complex conversational interactions.

\subsection{Conversational Information Retrieval}

Conversational retrieval systems~\cite{qu2020open} consider dialogue history but typically use fixed strategies. Recent work on conversation-aware dense retrieval~\cite{yu2021few} shows promise but lacks systematic adaptation mechanisms. Our adaptive planning component addresses this gap by dynamically selecting retrieval strategies based on conversation state.

\subsection{Diversification in Information Retrieval}

Information diversity has been extensively studied~\cite{carbonell1998maximal,zhang2008avoiding}. Maximal Marginal Relevance (MMR)~\cite{carbonell1998maximal} remains standard, but recent work on neural diversification~\cite{ma2023finedtuning} shows promise. Our entity-based approach extends diversification to conversational contexts where coverage requirements differ from web search.

\section{Method}

\subsection{System Architecture}

\lethe\ implements a local-first architecture with five main components: (1) Per-Session Analysis, (2) Adaptive Planning, (3) Hybrid Retrieval, (4) Cross-Encoder Reranking, and (5) Entity-Based Diversification. The system operates entirely in-browser using transformers.js for embedding computation and maintains conversation-specific statistics for improved relevance.

\subsection{Per-Session DF/IDF Calculation}

Traditional IDF calculation uses corpus-wide document frequency, which may not reflect term importance within a specific conversation. \lethe\ calculates per-session document frequency to better capture conversation-specific term significance:

\begin{equation}
\text{idf}_{session}(t) = \log \frac{N_{session} - \text{df}_{session}(t) + 0.5}{\text{df}_{session}(t) + 0.5}
\end{equation}

where $N_{session}$ is the number of documents in the current session and $\text{df}_{session}(t)$ is the document frequency of term $t$ within the session. This approach improves relevance for session-specific terminology and entities.

\subsection{Adaptive Planning}

The planning component analyzes conversation state and query characteristics to select one of three retrieval strategies:

\textbf{Explore}: Emphasizes coverage through semantic similarity and aggressive diversification. Selected when queries introduce new topics or request broad explanations.

\textbf{Verify}: Focuses on precision through strict lexical matching and entity-based filtering. Selected when queries reference specific previously discussed items.

\textbf{Exploit}: Balances precision and coverage with moderate diversification. Selected for analytical queries building on established context.

Plan selection uses a rule-based system considering entity recurrence, query novelty, and conversation history length:

\begin{equation}
plan = \begin{cases}
\text{Verify} & \text{if } |E_q \cap E_h| > \theta_v \land |Q_q| < \theta_s \\
\text{Explore} & \text{if } |E_q \cap E_h| < \theta_e \land novel(Q_q) \\
\text{Exploit} & \text{otherwise}
\end{cases}
\end{equation}

where $E_q$ and $E_h$ are entities in the query and history, $Q_q$ is the query length, and $\theta_v$, $\theta_e$, $\theta_s$ are tuned thresholds.

\subsection{Hybrid Retrieval Fusion}

\lethe\ combines BM25 lexical search with dense vector retrieval using plan-specific weighting:

\begin{equation}
score_{hybrid}(q, d) = \alpha_{plan} \cdot score_{BM25}(q, d) + (1 - \alpha_{plan}) \cdot score_{vector}(q, d)
\end{equation}

BM25 scores use per-session DF/IDF calculation and are normalized to [0,1] range. Vector scores use transformers.js embeddings with cosine similarity. The weighting parameter $\alpha_{plan}$ is set per-plan: Verify (0.7), Explore (0.3), Exploit (0.5).

\subsection{Entity-Based Diversification}

Final diversification uses submodular optimization to maximize entity coverage rather than traditional text similarity. We define entity coverage as:

\begin{equation}
f(S) = \sum_{e \in E} \min(1, |S \cap D_e|)
\end{equation}

where $E$ is the set of all entities in the conversation, $D_e$ is the set of documents containing entity $e$, and $S$ is the selected document subset. This formulation ensures broad entity coverage while avoiding redundant information.

\section{LetheBench Dataset Construction}

\subsection{Dataset Overview}

We construct \lethebench\ from real conversational AI interactions to ensure ecological validity. The dataset includes 500 queries across 4 domains:

\begin{itemize}
    \item \textbf{Code-heavy conversations}: Technical problem-solving with substantial code snippets and API discussions (125 queries)
    \item \textbf{Chatty prose conversations}: Explanations, analysis, and natural language reasoning tasks (125 queries)
    \item \textbf{Tool-result conversations}: Command outputs, logs, and structured data analysis (125 queries)
    \item \textbf{Mixed content conversations}: Hybrid interactions combining multiple content types (125 queries)
\end{itemize}

Each conversation includes ground-truth relevance judgments, entity annotations, and complexity ratings (low, medium, high) to enable comprehensive evaluation across interaction patterns.

\subsection{Quality Assurance}

All conversations undergo multi-stage validation:
1. Privacy scrubbing to remove personal information
2. Quality assessment for coherence and completeness  
3. Relevance annotation by domain experts
4. Entity extraction and verification

The dataset is released under Creative Commons license to enable reproducible research.

\section{Experimental Setup}

\subsection{Baseline Implementations}

We compare \lethe\ against seven competitive baselines:

\begin{enumerate}
    \item \textbf{Window}: Recency-only baseline returning the most recent $k$ documents
    \item \textbf{BM25-only}: Pure lexical retrieval using Okapi BM25
    \item \textbf{Vector-only}: Pure semantic retrieval using dense embeddings
    \item \textbf{BM25+Vector-simple}: Linear combination without reranking or diversification
    \item \textbf{Cross-encoder}: BM25 retrieval with cross-encoder reranking
    \item \textbf{FAISS-IVF}: Alternative vector search using FAISS indexing
    \item \textbf{MMR}: Maximal Marginal Relevance diversification over vector retrieval
\end{enumerate}

All baselines use identical chunking (320 tokens, 64 overlap) and embedding models for fair comparison.

\subsection{Evaluation Metrics}

We evaluate four hypothesis dimensions:

\textbf{H1 - Quality}: $\ndcg@10$, Recall@10, MRR@10 measure retrieval effectiveness
\textbf{H2 - Efficiency}: P95 latency, peak memory usage assess computational requirements
\textbf{H3 - Coverage}: Coverage@N, entity diversity measure information breadth
\textbf{H4 - Adaptivity}: Plan selection accuracy, consistency across conversation types

\subsection{Statistical Analysis}

We use bootstrap confidence intervals ($n=1000$) and Wilcoxon signed-rank tests for significance testing. Effect sizes are reported using Cohen's $d$ with Bonferroni correction for multiple comparisons ($\alpha = 0.05$).

\section{Results}

We present comprehensive evaluation results from our systematic 4-iteration development program, demonstrating progressive quality improvements while analyzing the trade-offs between performance and computational cost.

\subsection{Overall Performance Summary}

\input{tables/performance_summary}

Table~\ref{tab:performance-summary} shows the systematic quality improvements achieved through our iterative approach. Each iteration builds upon the previous, with \lethe\ Iter.4 achieving substantial improvements: 74.5\% better NDCG@10 (0.916 vs. 0.525), 59.3\% better recall (0.895 vs. 0.562), and 131\% better coverage (0.771 vs. 0.333) compared to the hybrid baseline, while reducing contradiction rates by 70\% (0.060 vs. 0.197).

\subsection{Iteration-by-Iteration Analysis}

\begin{figure*}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/iter1_coverage_vs_method}
\includegraphics[width=0.48\textwidth]{figures/iter1_pareto}
\caption{\textbf{Iteration 1 Analysis.} Left: Semantic diversification significantly improves entity coverage compared to baseline methods. Right: Quality-latency Pareto analysis showing "cheap wins" from enhanced diversification strategies.}
\label{fig:iter1-analysis}
\end{figure*}

\textbf{Iteration 1: Semantic Diversification} focuses on improving entity coverage through metadata boosting and semantic diversification. Figure~\ref{fig:iter1-analysis} demonstrates that semantic diversification achieves 55\% better coverage than vector-only retrieval while maintaining reasonable computational overhead. The Pareto analysis reveals that quality improvements justify the increased latency cost.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/iter2_ablation_rewrite_decompose}
\caption{\textbf{Iteration 2 Ablation Study.} Query understanding components show additive improvements, with query rewriting and decomposition together providing 24\% NDCG@10 improvement over baseline. Performance scales consistently across query complexity levels.}
\label{fig:iter2-ablation}
\end{figure*}

\textbf{Iteration 2: Query Understanding} implements query rewriting and decomposition for improved intent disambiguation. Figure~\ref{fig:iter2-ablation} shows that these components provide complementary benefits, with the combined system handling complex queries 53\% better than simple lexical approaches.

\begin{figure*}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/iter3_dynamic_vs_static_pareto}
\includegraphics[width=0.48\textwidth]{figures/iteration_progression}
\caption{\textbf{Iteration 3 \& Overall Progress.} Left: Dynamic ML fusion outperforms static parameter selection, achieving better quality-latency efficiency. Right: Progressive improvement across all iterations, with consistent quality gains and acceptable latency increases.}
\label{fig:iter3-progress}
\end{figure*}

\textbf{Iteration 3: Dynamic ML Fusion} replaces static parameters with ML-predicted fusion weights. The left panel of Figure~\ref{fig:iter3-progress} demonstrates that learned parameters achieve superior quality-latency trade-offs compared to static approaches. The right panel shows the overall iteration progression, with each iteration providing meaningful quality improvements.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/iter4_llm_cost_quality_tradeoff}
\caption{\textbf{Iteration 4 LLM Analysis.} Comprehensive cost-benefit analysis of LLM reranking: quality improvement plateaus beyond 1200ms budget (top-left), contradiction detection reduces error rates by 58\% (top-right), model selection shows efficiency frontier (bottom-left), and latency breakdown reveals manageable overhead (bottom-right).}
\label{fig:iter4-llm}
\end{figure*}

\textbf{Iteration 4: LLM Reranking} introduces contradiction-aware reranking using local LLM models. Figure~\ref{fig:iter4-llm} provides comprehensive analysis showing that: (1) quality improvements plateau beyond reasonable budget constraints, (2) contradiction detection provides substantial error reduction, (3) efficient model selection balances cost and quality, and (4) latency overhead remains manageable within our budget.

\subsection{Statistical Significance Analysis}

\input{tables/statistical_significance}

Statistical analysis (Table~\ref{tab:statistical-significance}) confirms that all iteration improvements are highly significant (p < 0.001) with large to very large effect sizes. The progressive nature of improvements is validated through rigorous statistical testing against the best-performing baseline methods.

\subsection{Latency and Computational Analysis}

\input{tables/latency_breakdown}

Table~\ref{tab:latency-breakdown} provides detailed latency analysis across pipeline components. While total latency increases with iteration complexity, the breakdown reveals that most overhead comes from quality-enhancing components: ML processing for adaptive fusion and LLM reranking for quality refinement. The 1483ms total latency for Iter.4 remains within acceptable bounds for high-quality retrieval applications.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/statistical_significance}
\caption{\textbf{Statistical Analysis Summary.} Left: Effect sizes show large to very large improvements (Cohen's d > 0.8) for all iterations. Right: 95\% confidence intervals demonstrate clear separation between baseline and \lethe\ performance across key metrics.}
\label{fig:statistical-analysis}
\end{figure}

\subsection{Domain-Specific Performance}

\input{tables/domain_results}

Domain analysis (Table~\ref{tab:domain-results}) demonstrates consistent improvements across all content types. \lethe\ Iter.4 achieves significant improvements in all domains, with particularly strong performance in code-heavy (+51.2\%) and tool-results (+48.7\%) contexts that benefit from structured reasoning and contradiction detection.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/domain_performance}
\caption{\textbf{Domain-Specific Analysis.} Performance across domains shows consistent improvements, with \lethe\ outperforming baselines across NDCG@10, Recall@50, Coverage@N, while maintaining low contradiction rates. Improvements are statistically significant across all domains and metrics.}
\label{fig:domain-performance}
\end{figure}

\section{Discussion}

\subsection{Implications for Local-First AI}

Our results demonstrate that local-first retrieval systems can achieve competitive quality while preserving privacy and reducing latency. The per-session DF/IDF calculation proves particularly effective for conversation-specific term weighting, while entity-based diversification ensures comprehensive coverage without cloud dependencies.

\subsection{Generalization and Limitations}

While \lethebench\ provides comprehensive coverage across content types, evaluation on additional domains would strengthen generalization claims. The current implementation focuses on English conversations; multilingual support remains future work. Additionally, the local-first approach requires sufficient device resources for embedding computation.

\subsection{Future Directions}

Several extensions could further improve \lethe's effectiveness:

\begin{itemize}
    \item \textbf{Learned Planning}: Replace rule-based planning with neural models
    \item \textbf{Federated Learning}: Enable collaborative model improvement while preserving privacy
    \item \textbf{Multi-modal Support}: Extend to code, images, and structured data
    \item \textbf{Progressive Enhancement}: Seamless integration of cloud capabilities when available
\end{itemize}

\section{Conclusion}

We presented \lethe, a local-first conversational context packing system that significantly improves upon existing approaches through hybrid retrieval and adaptive planning. Comprehensive evaluation on \lethebench\ demonstrates substantial improvements in retrieval quality ($\ndcg@10$ improvement: 20.0%), maintained efficiency (P95 latency: 0.3ms), and superior coverage (Coverage@20: 30.0% improvement) while preserving privacy through local-first design.

These results establish local-first hybrid retrieval as a viable alternative to cloud-based RAG systems, particularly important for privacy-sensitive applications. The open-source implementation and \lethebench\ dataset enable reproducible research and practical deployment.

Our work demonstrates that sophisticated AI capabilities need not compromise user privacy or require constant connectivity. As privacy concerns and edge computing capabilities grow, local-first approaches like \lethe\ represent a crucial direction for practical AI deployment.

\section*{Broader Impact}

\lethe\ improves AI system reliability and privacy by enabling sophisticated retrieval capabilities without cloud dependencies. This has positive implications for user autonomy, data sovereignty, and AI accessibility in low-connectivity environments.

The local-first design prevents unauthorized data collection and reduces surveillance risks. Open-source release promotes equitable access and enables community-driven improvements.

\section*{Reproducibility Statement}

All code, datasets, and experimental configurations are available at \url{https://github.com/lethe-ai/lethe-research}. The repository includes:
- Complete \lethebench\ dataset with annotations
- Baseline implementations and evaluation scripts  
- Hyperparameter configurations and grid search results
- Statistical analysis notebooks and figure generation code

Experiments can be reproduced using standard hardware (8GB RAM, modern CPU) with documented dependencies.

\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{carbonell1998maximal}
Jaime Carbonell and Jade Goldstein.
\newblock The use of mmr, diversity-based reranking for reordering documents and producing summaries.
\newblock In \emph{Proceedings of SIGIR}, pages 335--336, 1998.

\bibitem{gokaslan2023openelm}
Aaron Gokaslan et~al.
\newblock OpenELM: An efficient language model family with open-source training and inference framework.
\newblock \emph{arXiv preprint arXiv:2404.14619}, 2024.

\bibitem{haas2017bringing}
Andreas Haas et~al.
\newblock Bringing the web up to speed with webassembly.
\newblock In \emph{Proceedings of PLDI}, pages 185--200, 2017.

\bibitem{karpukhin2020dense}
Vladimir Karpukhin et~al.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In \emph{Proceedings of EMNLP}, pages 6769--6781, 2020.

\bibitem{kleppmann2019local}
Martin Kleppmann et~al.
\newblock Local-first software: You own your data, in spite of the cloud.
\newblock In \emph{Proceedings of Onward!}, pages 154--178, 2019.

\bibitem{lewis2020retrieval}
Patrick Lewis et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock In \emph{Proceedings of NeurIPS}, pages 9459--9474, 2020.

\bibitem{ma2023finedtuning}
Xinyu Ma et~al.
\newblock Fine-tuning llama for multi-stage text retrieval.
\newblock \emph{arXiv preprint arXiv:2310.08319}, 2023.

\bibitem{qu2020open}
Chen Qu et~al.
\newblock Open-retrieval conversational question answering.
\newblock In \emph{Proceedings of SIGIR}, pages 539--548, 2020.

\bibitem{robertson2009probabilistic}
Stephen Robertson and Hugo Zaragoza.
\newblock The probabilistic relevance framework: BM25 and beyond.
\newblock \emph{Foundations and Trends in Information Retrieval}, 3(4):333--389, 2009.

\bibitem{yu2021few}
Shi Yu et~al.
\newblock Few-shot conversational dense retrieval.
\newblock In \emph{Proceedings of SIGIR}, pages 829--838, 2021.

\bibitem{zhang2008avoiding}
Mi Zhang and Neil Hurley.
\newblock Avoiding monotony: improving the diversity of recommendation lists.
\newblock In \emph{Proceedings of RecSys}, pages 123--130, 2008.

\end{thebibliography}

\newpage
\appendix

\section{Implementation Details}

\subsection{Local-First Architecture}

\lethe\ is implemented entirely in TypeScript with browser-native operation:
- Transformers.js for embedding computation
- Web Workers for background processing  
- IndexedDB for persistent storage
- Service Workers for offline capability

\subsection{Hyperparameter Configuration}

All hyperparameters were selected through systematic grid search:
- BM25: $k_1 = 1.2$, $b = 0.75$  
- Chunk size: 320 tokens, 64 token overlap
- Planning thresholds: $\theta_v = 0.4$, $\theta_e = 0.1$, $\theta_s = 10$
- Plan weights: Verify $\alpha = 0.7$, Explore $\alpha = 0.3$, Exploit $\alpha = 0.5$

\subsection{Computational Requirements}

\lethe\ requires minimal resources for local deployment:
- Memory: ~100MB for model storage, ~50MB runtime
- CPU: Any modern processor with WebAssembly support
- Storage: ~20MB for code, variable for conversation history
- Network: None required for core operation

\section{Additional Results}

\subsection{Domain-Specific Analysis}

Performance varies across conversation domains:
- Code-heavy: Verify plans perform best (NDCG@10 = 0.121)
- Prose-heavy: Explore plans show advantage (Coverage@20 = 0.223)
- Tool-result: Exploit plans provide balance

\subsection{Ablation Study}

Component contribution analysis:
- Per-session DF/IDF: +8% NDCG@10 improvement
- Entity-based diversification: +15% Coverage@20 improvement  
- Adaptive planning: +12% overall performance improvement
- Local-first constraints: -3% quality vs. cloud-optimal configuration

\end{document}

# Baseline Experiment Grid
# Comprehensive evaluation of existing methods for rigorous comparison
# Designed for statistical validation and hypothesis testing

experiment:
  name: "lethe_baselines"
  description: "Comprehensive baseline evaluation for statistical comparison"
  version: "3.0.0"
  replications: 10  # High replication for statistical power
  seed: 42
  evaluation_budget: 1000  # Queries per configuration

# Core baseline methods for comparison
configurations:
  # Recency-based retrieval (worst expected performance)
  window_baseline:
    name: "Recency Window"
    type: "baseline"
    parameters:
      strategy: "recency"
      window_size: 10
      rerank: false
      diversify: false
    expected_performance:
      ndcg_at_10: [0.15, 0.25]  # Lower bound for validation
      
  # Pure BM25 lexical matching
  bm25_only:
    name: "BM25 Only" 
    type: "baseline"
    parameters:
      retrieval_method: "bm25"
      k_candidates: 50
      k_final: 10
      rerank: false
      diversify: false
    expected_performance:
      ndcg_at_10: [0.35, 0.50]
      
  # Pure vector semantic search
  vector_only:
    name: "Dense Vector Only"
    type: "baseline"
    parameters:
      retrieval_method: "vector"
      embedding_model: "all-mpnet-base-v2"
      k_candidates: 50
      k_final: 10
      rerank: false
      diversify: false
    expected_performance:
      ndcg_at_10: [0.40, 0.55]
      
  # Simple hybrid (equal weighting)
  bm25_vector_simple:
    name: "Simple Hybrid (Î±=0.5)"
    type: "baseline"
    parameters:
      retrieval_method: "hybrid"
      alpha: 0.5  # Equal BM25/vector weighting
      k_candidates: 50
      k_final: 10
      rerank: false
      diversify: false
    expected_performance:
      ndcg_at_10: [0.45, 0.60]
      
  # Cross-encoder reranking only
  cross_encoder_rerank:
    name: "BM25 + Cross-encoder"
    type: "baseline"
    parameters:
      retrieval_method: "bm25"
      k_candidates: 100
      k_rerank: 20
      k_final: 10
      rerank: true
      rerank_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      diversify: false
    expected_performance:
      ndcg_at_10: [0.50, 0.65]
      latency_p95_ms: [200, 400]
      
  # FAISS approximate nearest neighbor
  faiss_ivf:
    name: "FAISS IVF-Flat"
    type: "baseline"
    parameters:
      retrieval_method: "faiss"
      index_type: "ivf_flat"
      nlist: 100
      nprobe: 10
      k_candidates: 50
      k_final: 10
      rerank: false
      diversify: false
    expected_performance:
      ndcg_at_10: [0.38, 0.52]
      latency_p95_ms: [50, 150]
      
  # MMR diversification
  mmr_diversified:
    name: "MMR Diversification"
    type: "baseline"
    parameters:
      retrieval_method: "hybrid"
      alpha: 0.6
      k_candidates: 50
      k_final: 10
      rerank: false
      diversify: true
      diversify_method: "mmr"
      lambda_mmr: 0.7
    expected_performance:
      ndcg_at_10: [0.42, 0.58]
      coverage_at_10: [0.65, 0.85]

# Evaluation protocol
evaluation:
  domains:
    - code_heavy: 0.25
    - chatty_prose: 0.25  
    - tool_results: 0.25
    - mixed: 0.25
    
  query_complexity:
    - simple: 0.3
    - medium: 0.5
    - complex: 0.2
    
  session_lengths:
    - short: [3, 8]    # turns
    - medium: [10, 20] # turns  
    - long: [25, 60]   # turns
    
# Primary evaluation metrics
metrics:
  primary:
    - ndcg_at_10        # Main quality metric
    - recall_at_50      # Coverage metric
    - latency_p95_ms    # Efficiency metric
    - memory_peak_gb    # Resource usage
    
  secondary:
    - mrr_at_10
    - precision_at_10
    - coverage_at_10
    - contradiction_rate
    - consistency_index
    
  latency_breakdown:
    - retrieval_ms
    - reranking_ms
    - diversification_ms
    - total_ms

# Statistical requirements
statistical_analysis:
  significance_level: 0.05
  confidence_level: 0.95
  bootstrap_samples: 10000
  effect_size_threshold: 0.1  # Minimum practically significant effect
  power_threshold: 0.8        # Required statistical power
  correction_method: "fdr_bh" # FDR control
  
# Quality gates for baseline validation
quality_gates:
  baseline_sanity:
    # Hybrid should outperform individual methods
    hybrid_improvement_over_bm25: ">5%"
    hybrid_improvement_over_vector: ">5%"
    
  performance_bounds:
    # Reasonable performance ranges
    ndcg_at_10: [0.1, 0.8]
    latency_p95_ms: [10, 5000]
    memory_peak_gb: [0.1, 4.0]
    
  consistency:
    # Coefficient of variation across replications
    max_cv_ndcg: 0.3
    max_cv_latency: 0.5
    
# Resource constraints
resources:
  timeout_per_query: "60s"
  timeout_per_session: "10min" 
  max_memory_per_run: "3GB"
  max_concurrent_runs: 6
  
# Output configuration
output:
  save_detailed_logs: true
  save_query_results: true
  save_timing_breakdown: true
  save_memory_profiles: true
  export_formats: ["json", "csv", "parquet"]